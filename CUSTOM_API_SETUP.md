# ğŸ”Œ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… API Ø³ÙØ§Ø±Ø´ÛŒ

## ğŸ¯ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† Ù…Ø®ØªÙ„Ù

---

## âœ… Ú¯Ø§Ù… 1: ÙˆÛŒØ±Ø§ÛŒØ´ `self_development_spec.yaml`

### Ø¨Ø±Ø§ÛŒ API Ø³ÙØ§Ø±Ø´ÛŒ Ø´Ù…Ø§:

```yaml
llm:
  mode: "online"
  
  custom_api:
    enabled: true
    base_url: "https://your-api-server.com/v1"  # ğŸ”§ Ø¢Ø¯Ø±Ø³ Ø³Ø±ÙˆØ± Ø´Ù…Ø§
    api_key_env: "CUSTOM_API_KEY"
    model: "claude-sonnet-4-20250514"  # Sonnet 4.5
    
    timeout: 300
    retry: 3
    
    # Headers Ø§Ø¶Ø§ÙÛŒ (Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ø¯)
    custom_headers:
      "X-API-Version": "2024-10"
      # "X-Custom-Header": "value"
  
  online:
    use_cache: true  # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù‡Ø²ÛŒÙ†Ù‡
    max_tokens: 3000
    temperature: 0.7
  
  # Fallback ØºÛŒØ±ÙØ¹Ø§Ù„
  fallback_online: false
  fallback_to_mcp: false

# Ú©Ù†ØªØ±Ù„ Ù‡Ø²ÛŒÙ†Ù‡
cost_control:
  enabled: true
  max_cost_per_task: 0.10  # Ø¯Ù„Ø§Ø±
  max_total_cost: 1.00     # Ø¯Ù„Ø§Ø± - Ø¨Ø±Ø§ÛŒ 18 task
  warn_threshold: 0.50
  
  max_input_tokens: 2000
  max_output_tokens: 3000
```

---

## ğŸŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† Ù…Ø­Ø¨ÙˆØ¨

### 1ï¸âƒ£ OpenRouter (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "https://openrouter.ai/api/v1"
    api_key_env: "OPENROUTER_API_KEY"
    model: "anthropic/claude-sonnet-4-20250514"
    
    custom_headers:
      "HTTP-Referer": "https://your-site.com"
      "X-Title": "Auto-Dev-LLM"
```

**Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡:**
```
Sonnet 4.5 Ø¯Ø± OpenRouter:
Input:  $3.00 / 1M tokens
Output: $15.00 / 1M tokens

Ú©Ù„ Ù¾Ø±ÙˆÚ˜Ù‡: $0.76
```

### 2ï¸âƒ£ Together.ai

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "https://api.together.xyz/v1"
    api_key_env: "TOGETHER_API_KEY"
    model: "anthropic/claude-sonnet-4-20250514"
```

### 3ï¸âƒ£ Groq (Ø³Ø±ÛŒØ¹ Ùˆ Ø§Ø±Ø²Ø§Ù†)

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "https://api.groq.com/openai/v1"
    api_key_env: "GROQ_API_KEY"
    model: "llama-3.1-70b-versatile"  # Ù…Ø¯Ù„ Ø±Ø§ÛŒÚ¯Ø§Ù†
```

**Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡:** Ø±Ø§ÛŒÚ¯Ø§Ù†! ğŸ‰

### 4ï¸âƒ£ Anthropic Ù…Ø³ØªÙ‚ÛŒÙ…

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "https://api.anthropic.com/v1"
    api_key_env: "ANTHROPIC_API_KEY"
    model: "claude-sonnet-4-20250514"
    
    # Ø¨Ø±Ø§ÛŒ Anthropic Ø¨Ø§ÛŒØ¯ endpoint Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯
    endpoints:
      chat: "/messages"
```

### 5ï¸âƒ£ Azure OpenAI

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "https://YOUR-RESOURCE.openai.azure.com"
    api_key_env: "AZURE_OPENAI_KEY"
    model: "gpt-4"
    
    custom_headers:
      "api-key": "${AZURE_OPENAI_KEY}"
```

### 6ï¸âƒ£ Ø³Ø±ÙˆØ± Ù…Ø­Ù„ÛŒ (LM Studio, Ollama)

```yaml
llm:
  custom_api:
    enabled: true
    base_url: "http://localhost:1234/v1"  # LM Studio
    # ÛŒØ§: "http://localhost:11434/v1"  # Ollama
    api_key_env: "DUMMY_KEY"  # Ù‡Ø± Ú†ÛŒØ²ÛŒ
    model: "llama-3.1-8b"
```

**Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡:** Ø±Ø§ÛŒÚ¯Ø§Ù†! ğŸ‰

---

## âš™ï¸ Ú¯Ø§Ù… 2: ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ

### Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ `.env`:

```bash
# Ø¨Ø±Ø§ÛŒ API Ø³ÙØ§Ø±Ø´ÛŒ
CUSTOM_API_KEY=your-api-key-here

# ÛŒØ§ Ø¨Ø±Ø§ÛŒ OpenRouter
OPENROUTER_API_KEY=sk-or-v1-xxxxx

# ÛŒØ§ Ø¨Ø±Ø§ÛŒ Groq
GROQ_API_KEY=gsk_xxxxx

# ÛŒØ§ Ø¨Ø±Ø§ÛŒ Together
TOGETHER_API_KEY=xxxxx
```

### Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø± Ú©Ø¯:

```bash
# Linux/Mac
export CUSTOM_API_KEY="your-key"

# Windows
set CUSTOM_API_KEY=your-key

# ÛŒØ§ Ø¨Ø§ dotenv
pip install python-dotenv
```

---

## ğŸ§ª Ú¯Ø§Ù… 3: ØªØ³Øª Ø§ØªØµØ§Ù„

### Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ³Øª Ø³Ø±ÛŒØ¹:

```python
# test_custom_api.py
import asyncio
import os
from src.llm.llama_wrapper import LLMWrapper, LLMRequest

async def test_connection():
    config = {
        'mode': 'custom',
        'custom_api': {
            'enabled': True,
            'base_url': 'https://your-api-server.com/v1',
            'api_key_env': 'CUSTOM_API_KEY',
            'model': 'claude-sonnet-4-20250514'
        },
        'online': {
            'use_cache': True
        },
        'cost_control': {
            'max_total_cost': 0.10  # ÙÙ‚Ø· 10 Ø³Ù†Øª Ø¨Ø±Ø§ÛŒ ØªØ³Øª
        }
    }
    
    wrapper = LLMWrapper(config)
    
    # ØªØ³Øª Ø³Ø§Ø¯Ù‡
    request = LLMRequest(
        prompt="Ø¨Ú¯Ùˆ Ø³Ù„Ø§Ù…",
        max_tokens=50
    )
    
    response = await wrapper.generate(request)
    
    if response.success:
        print("âœ… Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚!")
        print(f"ğŸ“ Ù¾Ø§Ø³Ø®: {response.content}")
        print(f"ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡: ${response.cost:.4f}")
        print(f"ğŸ¯ Tokens: {response.tokens_used}")
    else:
        print(f"âŒ Ø®Ø·Ø§: {response.error}")

if __name__ == "__main__":
    asyncio.run(test_connection())
```

**Ø§Ø¬Ø±Ø§:**
```bash
python test_custom_api.py
```

---

## ğŸ’° Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡ Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÙˆØ± Ø´Ù…Ø§

### ÙØ±Ù…ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡:

```python
def calculate_project_cost(
    input_price_per_1m: float,   # Ù‚ÛŒÙ…Øª input Ø´Ù…Ø§
    output_price_per_1m: float,  # Ù‚ÛŒÙ…Øª output Ø´Ù…Ø§
    num_tasks: int = 18
):
    # Ù‡Ø± task
    input_tokens = 1500
    output_tokens = 2500
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡
    input_cost = (input_tokens * num_tasks * input_price_per_1m) / 1_000_000
    output_cost = (output_tokens * num_tasks * output_price_per_1m) / 1_000_000
    
    total = input_cost + output_cost
    
    return {
        'input_cost': input_cost,
        'output_cost': output_cost,
        'total_cost': total,
        'per_task': total / num_tasks
    }

# Ù…Ø«Ø§Ù„ Ø¨Ø§ Sonnet 4.5
result = calculate_project_cost(
    input_price_per_1m=3.00,
    output_price_per_1m=15.00
)

print(f"Ú©Ù„ Ù‡Ø²ÛŒÙ†Ù‡: ${result['total_cost']:.2f}")
print(f"Ù‡Ø²ÛŒÙ†Ù‡ Ù‡Ø± task: ${result['per_task']:.3f}")
```

---

## ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø³Ø±ÙˆÛŒØ³â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù†

| Provider | Model | Input | Output | Total (18 tasks) |
|----------|-------|-------|--------|------------------|
| **OpenRouter** | Sonnet 4.5 | $3.00 | $15.00 | **$0.76** â­ |
| **Anthropic** | Sonnet 4.5 | $3.00 | $15.00 | **$0.76** |
| **OpenAI** | GPT-4o | $2.50 | $10.00 | **$0.59** |
| **Groq** | Llama 3.1 70B | FREE | FREE | **$0.00** ğŸ‰ |
| **Together** | Llama 3.1 70B | $0.60 | $0.60 | **$0.13** |
| **Local** | Any | FREE | FREE | **$0.00** ğŸ‰ |

---

## ğŸ”§ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ

### Ù…Ø´Ú©Ù„ 1: "Connection refused"

```bash
# Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø¯Ø±Ø³
curl https://your-api-server.com/v1/models

# Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø§ header
curl https://your-api-server.com/v1/chat/completions \
  -H "Authorization: Bearer $CUSTOM_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"claude-sonnet-4-20250514","messages":[{"role":"user","content":"hi"}],"max_tokens":10}'
```

### Ù…Ø´Ú©Ù„ 2: "Invalid API key"

```bash
# Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ
echo $CUSTOM_API_KEY

# ØªØ³Øª Ø¨Ø§ Python
python -c "import os; print(os.getenv('CUSTOM_API_KEY'))"
```

### Ù…Ø´Ú©Ù„ 3: "Model not found"

```bash
# Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
curl https://your-api-server.com/v1/models \
  -H "Authorization: Bearer $CUSTOM_API_KEY"
```

### Ù…Ø´Ú©Ù„ 4: "Rate limit exceeded"

Ø¯Ø± `self_development_spec.yaml`:
```yaml
scheduler:
  max_concurrent_tasks: 1  # Ú©Ø§Ù‡Ø´ Ø¨Ù‡ 1
  check_interval: 60  # ØµØ¨Ø± Ø¨ÛŒØ´ØªØ±

llm:
  custom_api:
    retry: 5  # ØªÙ„Ø§Ø´ Ø¨ÛŒØ´ØªØ±
```

---

## ğŸ¯ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡Ø²ÛŒÙ†Ù‡

### 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Cache

```yaml
llm:
  online:
    use_cache: true  # ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ 30-40%
```

### 2. Ú©Ø§Ù‡Ø´ Max Tokens

```yaml
cost_control:
  max_output_tokens: 2500  # Ø¨Ù‡ Ø¬Ø§ÛŒ 3000
```

### 3. Ø§Ø¬Ø±Ø§ÛŒ ØªØ¯Ø±ÛŒØ¬ÛŒ

```bash
# Ø§ÙˆÙ„ ÙÙ‚Ø· 1 feature
python main.py --batch --features git-automation

# Ø³Ù¾Ø³ Ø¨Ù‚ÛŒÙ‡
python main.py --batch --features version-control rollback-recovery
```

### 4. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø§Ø±Ø²Ø§Ù†â€ŒØªØ± Ø¨Ø±Ø§ÛŒ ØªØ³Øª

```yaml
llm:
  custom_api:
    model: "gpt-3.5-turbo"  # Ø¨Ø±Ø§ÛŒ ØªØ³Øª
    # model: "claude-sonnet-4-20250514"  # Ø¨Ø±Ø§ÛŒ production
```

---

## ğŸ“ˆ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù‡Ø²ÛŒÙ†Ù‡ Real-Time

```python
# Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§
from src.llm.llama_wrapper import LLMWrapper

# Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù‡Ø²ÛŒÙ†Ù‡
summary = wrapper.get_cost_summary()
print(f"ğŸ’° Ù‡Ø²ÛŒÙ†Ù‡ ØªØ§ Ú©Ù†ÙˆÙ†: ${summary['total_cost']}")
print(f"ğŸ“Š Ø¯Ø±ØµØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡: {summary['percentage']}%")
print(f"ğŸ’³ Ø¨Ø§Ù‚ÛŒÙ…Ø§Ù†Ø¯Ù‡: ${summary['remaining']}")
```

---

## âœ… Checklist Ù†Ù‡Ø§ÛŒÛŒ

- [ ] `base_url` Ø³Ø±ÙˆØ± Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± spec ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯ÛŒØ¯
- [ ] `api_key` Ø±Ø§ Ø¯Ø± `.env` ÛŒØ§ Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ø±Ø¯ÛŒØ¯
- [ ] `model` ØµØ­ÛŒØ­ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯ÛŒØ¯
- [ ] ØªØ³Øª Ø§ØªØµØ§Ù„ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ (`test_custom_api.py`)
- [ ] Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡ (`max_total_cost`) ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯
- [ ] Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø¬Ø±Ø§ Ù‡Ø³ØªÛŒØ¯! ğŸš€

---

## ğŸ‰ Ø§Ø¬Ø±Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ

```bash
# Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³ÙØ§Ø±Ø´ÛŒ Ø´Ù…Ø§
python bootstrap_self_dev.py
```

**Ø³ÛŒØ³ØªÙ… Ø§Ø² Ø³Ø±ÙˆØ± API Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ú©Ø±Ø¯!** âœ¨