"""
LLM Wrapper - Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§ (Custom API, MCP, Offline, Online)
"""

import asyncio
import aiohttp
import time
from typing import Optional, Dict, Any, List
from enum import Enum
from dataclasses import dataclass
import json
import os


class LLMProvider(Enum):
    """Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† LLM"""
    CUSTOM = "custom"
    MCP = "mcp"
    OFFLINE = "offline"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class LLMRequest:
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ LLM"""
    prompt: str
    max_tokens: int = 2048
    temperature: float = 0.7
    system_prompt: Optional[str] = None
    context: Optional[List[Dict[str, str]]] = None


@dataclass
class LLMResponse:
    """Ù¾Ø§Ø³Ø® Ø§Ø² LLM"""
    content: str
    model: str
    provider: LLMProvider
    tokens_used: int
    duration: float
    success: bool
    cost: float = 0.0  # Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø¢ÙˆØ±Ø¯ÛŒ
    error: Optional[str] = None


class CustomAPIClient:
    """Ú©Ù„Ø§ÛŒÙ†Øª Ø¨Ø±Ø§ÛŒ API Ø³ÙØ§Ø±Ø´ÛŒ"""
    
    def __init__(
        self,
        base_url: str,
        api_key: str,
        model: str,
        timeout: int = 300,
        retry: int = 3,
        custom_headers: Optional[Dict[str, str]] = None,
        use_cache: bool = True
    ):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.timeout = timeout
        self.retry = retry
        self.custom_headers = custom_headers or {}
        self.use_cache = use_cache
        
        # Ù‚ÛŒÙ…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Sonnet 4.5 (per million tokens)
        self.pricing = {
            'input': 3.00,
            'output': 15.00,
            'cache_write': 3.75,
            'cache_read': 0.30
        }
    
    def _calculate_cost(
        self,
        input_tokens: int,
        output_tokens: int,
        cache_hit: bool = False
    ) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡"""
        if cache_hit:
            input_cost = (input_tokens * self.pricing['cache_read']) / 1_000_000
        else:
            input_cost = (input_tokens * self.pricing['input']) / 1_000_000
        
        output_cost = (output_tokens * self.pricing['output']) / 1_000_000
        
        return input_cost + output_cost
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API Ø³ÙØ§Ø±Ø´ÛŒ"""
        start_time = time.time()
        
        # Ø³Ø§Ø®Øª headers
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            **self.custom_headers
        }
        
        # Ø³Ø§Ø®Øª messages
        messages = []
        if request.system_prompt:
            messages.append({
                "role": "system",
                "content": request.system_prompt
            })
        
        if request.context:
            messages.extend(request.context)
        
        messages.append({
            "role": "user",
            "content": request.prompt
        })
        
        # Ø³Ø§Ø®Øª payload
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature
        }
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† cache headers
        if self.use_cache:
            headers["anthropic-beta"] = "prompt-caching-2024-07-31"
            # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ system prompt Ø¨Ø±Ø§ÛŒ cache
            if messages and messages[0]["role"] == "system":
                messages[0]["cache_control"] = {"type": "ephemeral"}
        
        for attempt in range(self.retry):
            try:
                async with aiohttp.ClientSession() as session:
                    # URL Ú©Ø§Ù…Ù„
                    url = f"{self.base_url}/chat/completions"
                    
                    async with session.post(
                        url,
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.timeout)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            duration = time.time() - start_time
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ (OpenAI format)
                            if 'choices' in data:
                                content = data['choices'][0]['message']['content']
                                tokens = data.get('usage', {})
                                input_tokens = tokens.get('prompt_tokens', 0)
                                output_tokens = tokens.get('completion_tokens', 0)
                                total_tokens = tokens.get('total_tokens', input_tokens + output_tokens)
                            # ÛŒØ§ Anthropic format
                            elif 'content' in data:
                                content = data['content'][0]['text']
                                usage = data.get('usage', {})
                                input_tokens = usage.get('input_tokens', 0)
                                output_tokens = usage.get('output_tokens', 0)
                                total_tokens = input_tokens + output_tokens
                            else:
                                raise Exception("ÙØ±Ù…Øª Ù¾Ø§Ø³Ø® Ù†Ø§Ù…Ø¹ØªØ¨Ø±")
                            
                            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ø²ÛŒÙ†Ù‡
                            cache_hit = data.get('usage', {}).get('cache_read_input_tokens', 0) > 0
                            cost = self._calculate_cost(input_tokens, output_tokens, cache_hit)
                            
                            return LLMResponse(
                                content=content,
                                model=self.model,
                                provider=LLMProvider.CUSTOM,
                                tokens_used=total_tokens,
                                duration=duration,
                                success=True,
                                cost=cost
                            )
                        else:
                            error_text = await response.text()
                            raise Exception(f"API error {response.status}: {error_text}")
            
            except Exception as e:
                if attempt == self.retry - 1:
                    duration = time.time() - start_time
                    return LLMResponse(
                        content='',
                        model=self.model,
                        provider=LLMProvider.CUSTOM,
                        tokens_used=0,
                        duration=duration,
                        success=False,
                        cost=0.0,
                        error=str(e)
                    )
                await asyncio.sleep(2 ** attempt)  # Exponential backoff


class LLMWrapper:
    """Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.mode = config.get('mode', 'custom')
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§
        self.custom_client = None
        self.mcp_client = None
        self.offline_llm = None
        
        # Ø±Ø¯Ú¯ÛŒØ±ÛŒ Ù‡Ø²ÛŒÙ†Ù‡
        self.total_cost = 0.0
        self.max_total_cost = config.get('cost_control', {}).get('max_total_cost', 10.0)
        
        self._setup_clients()
    
    def _setup_clients(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§"""
        
        # Custom API (Ø§ÙˆÙ„ÙˆÛŒØª Ø§ÙˆÙ„)
        custom_config = self.config.get('custom_api', {})
        if custom_config.get('enabled') or self.mode == 'custom':
            api_key = os.getenv(custom_config.get('api_key_env', 'CUSTOM_API_KEY'))
            
            if api_key:
                self.custom_client = CustomAPIClient(
                    base_url=custom_config.get('base_url', 'http://localhost:8000'),
                    api_key=api_key,
                    model=custom_config.get('model', 'claude-sonnet-4-20250514'),
                    timeout=custom_config.get('timeout', 300),
                    retry=custom_config.get('retry', 3),
                    custom_headers=custom_config.get('custom_headers', {}),
                    use_cache=self.config.get('online', {}).get('use_cache', True)
                )
        
        # MCP (fallback)
        if self.config.get('fallback_to_mcp'):
            from llm.mcp_client import MCPClient
            mcp_config = self.config.get('mcp', {})
            self.mcp_client = MCPClient(
                api_url=mcp_config.get('api_url', 'http://localhost:5005'),
                timeout=mcp_config.get('timeout', 300),
                retry=mcp_config.get('retry', 2)
            )
    
    def check_cost_limit(self, estimated_cost: float) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡"""
        if self.total_cost + estimated_cost > self.max_total_cost:
            return False
        return True
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM"""
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡
        estimated_cost = 0.042  # ØªØ®Ù…ÛŒÙ†ÛŒ per task
        if not self.check_cost_limit(estimated_cost):
            return LLMResponse(
                content='',
                model='none',
                provider=LLMProvider.CUSTOM,
                tokens_used=0,
                duration=0,
                success=False,
                cost=0.0,
                error=f"Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ù‡Ø²ÛŒÙ†Ù‡ Ø±Ø³ÛŒØ¯Ù‡: ${self.total_cost:.3f} / ${self.max_total_cost}"
            )
        
        # ØªÙ„Ø§Ø´ Ø¨Ø§ Custom API
        if self.custom_client:
            response = await self.custom_client.generate(request)
            if response.success:
                self.total_cost += response.cost
                return response
            
            print(f"âš ï¸  Custom API Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {response.error}")
        
        # Fallback Ø¨Ù‡ MCP
        if self.config.get('fallback_to_mcp') and self.mcp_client:
            print("ğŸ”„ Fallback Ø¨Ù‡ MCP...")
            response = await self.mcp_client.generate(request)
            if response.success:
                return response
        
        # Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù†Ø¯
        return LLMResponse(
            content='',
            model='none',
            provider=LLMProvider.CUSTOM,
            tokens_used=0,
            duration=0,
            success=False,
            cost=0.0,
            error="Ù‡ÛŒÚ† LLM Ù…ÙˆÙÙ‚ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        )
    
    async def generate_code(
        self,
        task_description: str,
        file_path: str,
        context: Optional[str] = None
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© task Ø®Ø§Øµ"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ Ù…Ø§Ù‡Ø± Python Ù‡Ø³ØªÛŒØ¯.

Ù‚ÙˆØ§Ù†ÛŒÙ† Ù…Ù‡Ù…:
1. Ú©Ø¯ Ú©Ø§Ù…Ù„ØŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ùˆ Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
2. Ø§Ø² type hints Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. docstring Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø§Ù„Ø²Ø§Ù…ÛŒ Ø§Ø³Øª
4. error handling Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
5. Ú©Ø¯ ØªÙ…ÛŒØ² Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯ (PEP 8)
6. ÙÙ‚Ø· Ú©Ø¯ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† markdown ÛŒØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÛŒ
7. Ú©Ø¯ Ø¨Ø§ÛŒØ¯ self-contained Ø¨Ø§Ø´Ø¯ (Ù‡Ù…Ù‡ import Ù‡Ø§ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§)"""
        
        prompt = f"""Task: {task_description}

Target File: {file_path}

{f"Context:\n{context}\n" if context else ""}
Ù„Ø·ÙØ§Ù‹ Ú©Ø¯ Ú©Ø§Ù…Ù„ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯. ÙÙ‚Ø· Ú©Ø¯ PythonØŒ Ø¨Ø¯ÙˆÙ† ``` ÛŒØ§ markdown."""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=self.config.get('cost_control', {}).get('max_output_tokens', 3000),
            temperature=0.3
        )
        
        return await self.generate(request)
    
    async def generate_tests(
        self,
        code: str,
        file_path: str
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ú©Ø¯"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© ØªØ³Øªâ€ŒÙ†ÙˆÛŒØ³ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯.

Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ø§ pytest Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
2. Ù…ÙˆØ§Ø±Ø¯ Ù…Ø±Ø²ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡ÛŒØ¯
3. ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§Ø´Ù†Ø¯
4. Ø§Ø² fixtures Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
5. docstring Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªØ³Øª Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯"""
        
        prompt = f"""Ú©Ø¯ Ø²ÛŒØ± Ø±Ø§ ØªØ³Øª Ú©Ù†ÛŒØ¯:

{code}

Target Test File: {file_path}

ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ pytest Ú©Ø§Ù…Ù„ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯. ÙÙ‚Ø· Ú©Ø¯ Python."""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2048,
            temperature=0.3
        )
        
        return await self.generate(request)
    
    def get_cost_summary(self) -> Dict[str, Any]:
        """Ø®Ù„Ø§ØµÙ‡ Ù‡Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§"""
        return {
            'total_cost': round(self.total_cost, 3),
            'max_cost': self.max_total_cost,
            'remaining': round(self.max_total_cost - self.total_cost, 3),
            'percentage': round((self.total_cost / self.max_total_cost) * 100, 1)
        }


# ØªØ³Øª Ø³Ø±ÛŒØ¹
if __name__ == "__main__":
    async def test_custom_api():
        config = {
            'mode': 'custom',
            'custom_api': {
                'enabled': True,
                'base_url': 'https://your-api-server.com/v1',
                'api_key_env': 'CUSTOM_API_KEY',
                'model': 'claude-sonnet-4-20250514',
                'timeout': 300,
                'retry': 3
            },
            'online': {
                'use_cache': True
            },
            'cost_control': {
                'max_total_cost': 2.0,
                'max_output_tokens': 3000
            },
            'fallback_to_mcp': False
        }
        
        wrapper = LLMWrapper(config)
        
        # ØªØ³Øª ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯
        response = await wrapper.generate_code(
            task_description="Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÛŒØ¨ÙˆÙ†Ø§Ú†ÛŒ Ø¨Ø§ memoization",
            file_path="fibonacci.py"
        )
        
        if response.success:
            print(f"âœ… Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯!")
            print(f"ğŸ“Š Model: {response.model}")
            print(f"ğŸ’° Cost: ${response.cost:.4f}")
            print(f"â±ï¸  Duration: {response.duration:.2f}s")
            print(f"ğŸ¯ Tokens: {response.tokens_used}")
            print(f"\nğŸ“ Generated Code:\n{response.content[:300]}...")
            
            # Ø®Ù„Ø§ØµÙ‡ Ù‡Ø²ÛŒÙ†Ù‡
            summary = wrapper.get_cost_summary()
            print(f"\nğŸ’³ Cost Summary:")
            print(f"   Total: ${summary['total_cost']}")
            print(f"   Remaining: ${summary['remaining']}")
            print(f"   Used: {summary['percentage']}%")
        else:
            print(f"âŒ Ø®Ø·Ø§: {response.error}")
    
    asyncio.run(test_custom_api())"""
LLM Wrapper - Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§ (MCP, Offline, Online)
"""

import asyncio
import aiohttp
import time
from typing import Optional, Dict, Any, List
from enum import Enum
from dataclasses import dataclass
import json


class LLMProvider(Enum):
    """Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† LLM"""
    MCP = "mcp"
    OFFLINE = "offline"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class LLMRequest:
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ LLM"""
    prompt: str
    max_tokens: int = 2048
    temperature: float = 0.7
    system_prompt: Optional[str] = None
    context: Optional[List[Dict[str, str]]] = None


@dataclass
class LLMResponse:
    """Ù¾Ø§Ø³Ø® Ø§Ø² LLM"""
    content: str
    model: str
    provider: LLMProvider
    tokens_used: int
    duration: float
    success: bool
    error: Optional[str] = None


class MCPClient:
    """Ú©Ù„Ø§ÛŒÙ†Øª MCP Server"""
    
    def __init__(self, api_url: str, timeout: int = 300, retry: int = 3):
        self.api_url = api_url
        self.timeout = timeout
        self.retry = retry
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ MCP"""
        start_time = time.time()
        
        payload = {
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "system_prompt": request.system_prompt
        }
        
        for attempt in range(self.retry):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.api_url}/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.timeout)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            duration = time.time() - start_time
                            
                            return LLMResponse(
                                content=data.get('content', ''),
                                model=data.get('model', 'mcp-model'),
                                provider=LLMProvider.MCP,
                                tokens_used=data.get('tokens', 0),
                                duration=duration,
                                success=True
                            )
                        else:
                            error_text = await response.text()
                            raise Exception(f"MCP error: {response.status} - {error_text}")
            
            except Exception as e:
                if attempt == self.retry - 1:
                    duration = time.time() - start_time
                    return LLMResponse(
                        content='',
                        model='mcp-failed',
                        provider=LLMProvider.MCP,
                        tokens_used=0,
                        duration=duration,
                        success=False,
                        error=str(e)
                    )
                await asyncio.sleep(2 ** attempt)  # Exponential backoff


class OfflineLLM:
    """LLM Ø¢ÙÙ„Ø§ÛŒÙ† (LLaMA/StarCoder)"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
        try:
            # Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯
            # Ù…Ø«Ù„Ø§Ù‹ llama-cpp-python ÛŒØ§ transformers
            print(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø²: {self.model_path}")
            
            # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ llama.cpp ÛŒØ§ ctransformers
            # from llama_cpp import Llama
            # self.model = Llama(model_path=self.model_path)
            
            print("âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
            self.model = None
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ù…Ø¯Ù„ Ø¢ÙÙ„Ø§ÛŒÙ†"""
        if not self.model:
            return LLMResponse(
                content='',
                model='offline-not-loaded',
                provider=LLMProvider.OFFLINE,
                tokens_used=0,
                duration=0,
                success=False,
                error="Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"
            )
        
        start_time = time.time()
        
        try:
            # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
            # output = self.model(
            #     request.prompt,
            #     max_tokens=request.max_tokens,
            #     temperature=request.temperature
            # )
            
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            await asyncio.sleep(1)
            output = "# Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ LLM Ø¢ÙÙ„Ø§ÛŒÙ†\npass"
            
            duration = time.time() - start_time
            
            return LLMResponse(
                content=output,
                model='llama-3.1-7b',
                provider=LLMProvider.OFFLINE,
                tokens_used=len(output.split()),
                duration=duration,
                success=True
            )
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model='llama-failed',
                provider=LLMProvider.OFFLINE,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )


class OnlineLLM:
    """LLM Ø¢Ù†Ù„Ø§ÛŒÙ† (OpenAI/Anthropic)"""
    
    def __init__(self, provider: str, api_key: str, model: str):
        self.provider = provider
        self.api_key = api_key
        self.model = model
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ API Ø¢Ù†Ù„Ø§ÛŒÙ†"""
        start_time = time.time()
        
        if self.provider == "openai":
            return await self._generate_openai(request, start_time)
        elif self.provider == "anthropic":
            return await self._generate_anthropic(request, start_time)
        else:
            return LLMResponse(
                content='',
                model='unknown',
                provider=LLMProvider.OPENAI,
                tokens_used=0,
                duration=0,
                success=False,
                error=f"Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {self.provider}"
            )
    
    async def _generate_openai(self, request: LLMRequest, start_time: float) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ OpenAI API"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                messages = []
                if request.system_prompt:
                    messages.append({"role": "system", "content": request.system_prompt})
                
                if request.context:
                    messages.extend(request.context)
                
                messages.append({"role": "user", "content": request.prompt})
                
                payload = {
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature
                }
                
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        tokens = data['usage']['total_tokens']
                        duration = time.time() - start_time
                        
                        return LLMResponse(
                            content=content,
                            model=self.model,
                            provider=LLMProvider.OPENAI,
                            tokens_used=tokens,
                            duration=duration,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"OpenAI error: {response.status} - {error_text}")
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model=self.model,
                provider=LLMProvider.OPENAI,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )
    
    async def _generate_anthropic(self, request: LLMRequest, start_time: float) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Anthropic API"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "Content-Type": "application/json"
                }
                
                messages = []
                if request.context:
                    messages.extend(request.context)
                
                messages.append({"role": "user", "content": request.prompt})
                
                payload = {
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature
                }
                
                if request.system_prompt:
                    payload["system"] = request.system_prompt
                
                async with session.post(
                    "https://api.anthropic.com/v1/messages",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['content'][0]['text']
                        tokens = data['usage']['input_tokens'] + data['usage']['output_tokens']
                        duration = time.time() - start_time
                        
                        return LLMResponse(
                            content=content,
                            model=self.model,
                            provider=LLMProvider.ANTHROPIC,
                            tokens_used=tokens,
                            duration=duration,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"Anthropic error: {response.status} - {error_text}")
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model=self.model,
                provider=LLMProvider.ANTHROPIC,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )


class LLMWrapper:
    """Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.mode = config.get('mode', 'mcp')
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§
        self.mcp_client = None
        self.offline_llm = None
        self.online_llm = None
        
        self._setup_clients()
    
    def _setup_clients(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§"""
        # MCP
        if self.mode == 'mcp' or self.config.get('fallback_online'):
            mcp_config = self.config.get('mcp', {})
            self.mcp_client = MCPClient(
                api_url=mcp_config.get('api_url', 'http://localhost:5005'),
                timeout=mcp_config.get('timeout', 300),
                retry=mcp_config.get('retry', 3)
            )
        
        # Offline
        if self.mode == 'offline':
            offline_config = self.config.get('offline_model', {})
            self.offline_llm = OfflineLLM(
                model_path=offline_config.get('path', './models/model.gguf')
            )
        
        # Online (Fallback)
        if self.config.get('fallback_online'):
            online_config = self.config.get('online', {})
            import os
            api_key = os.getenv(online_config.get('api_key_env', 'OPENAI_API_KEY'))
            
            self.online_llm = OnlineLLM(
                provider=online_config.get('provider', 'openai'),
                api_key=api_key or '',
                model=online_config.get('model', 'gpt-4')
            )
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM"""
        
        # ØªÙ„Ø§Ø´ Ø¨Ø§ MCP
        if self.mode == 'mcp' and self.mcp_client:
            response = await self.mcp_client.generate(request)
            if response.success:
                return response
            
            print(f"âš ï¸  MCP Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {response.error}")
        
        # ØªÙ„Ø§Ø´ Ø¨Ø§ Offline
        if self.mode == 'offline' and self.offline_llm:
            response = await self.offline_llm.generate(request)
            if response.success:
                return response
            
            print(f"âš ï¸  Offline LLM Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {response.error}")
        
        # Fallback Ø¨Ù‡ Online
        if self.config.get('fallback_online') and self.online_llm:
            print("ğŸ”„ Fallback Ø¨Ù‡ API Ø¢Ù†Ù„Ø§ÛŒÙ†...")
            response = await self.online_llm.generate(request)
            return response
        
        # Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù†Ø¯
        return LLMResponse(
            content='',
            model='none',
            provider=LLMProvider.MCP,
            tokens_used=0,
            duration=0,
            success=False,
            error="Ù‡ÛŒÚ† LLM Ù…ÙˆÙÙ‚ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        )
    
    async def generate_code(
        self,
        task_description: str,
        file_path: str,
        context: Optional[str] = None
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© task Ø®Ø§Øµ"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ Ù…Ø§Ù‡Ø± Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ú©Ø¯ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.
Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. Ú©Ø¯ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù…Ù„ØŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ùˆ Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ Ø¨Ø§Ø´Ø¯
2. Ø§Ø² type hints Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. docstring Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
4. error handling Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
5. Ú©Ø¯ Ø¨Ø§ÛŒØ¯ ØªÙ…ÛŒØ² Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯ (PEP 8)
6. ÙÙ‚Ø· Ú©Ø¯ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÛŒ"""
        
        prompt = f"""Task: {task_description}
File: {file_path}

{"Context:\n" + context if context else ""}

Ù„Ø·ÙØ§Ù‹ Ú©Ø¯ Ú©Ø§Ù…Ù„ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=4096,
            temperature=0.3
        )
        
        return await self.generate(request)
    
    async def generate_tests(
        self,
        code: str,
        file_path: str
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ú©Ø¯"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© ØªØ³Øªâ€ŒÙ†ÙˆÛŒØ³ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯.
Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
2. Ø§Ø² pytest Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. Ù…ÙˆØ§Ø±Ø¯ Ù…Ø±Ø²ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡ÛŒØ¯
4. ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§Ø´Ù†Ø¯
5. docstring Ø¨Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯"""
        
        prompt = f"""Ú©Ø¯ Ø²ÛŒØ± Ø±Ø§ ØªØ³Øª Ú©Ù†ÛŒØ¯:

```python
{code}
```

File path: {file_path}

Ù„Ø·ÙØ§Ù‹ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ pytest Ú©Ø§Ù…Ù„ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2048,
            temperature=0.3
        )
        
        return await self.generate(request)
    
    async def review_code(self, code: str) -> LLMResponse:
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ø¯"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© code reviewer Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯.
Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ù‡ÛŒØ¯."""
        
        prompt = f"""Ú©Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:

```python
{code}
```

Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø±Ø§ Ù„ÛŒØ³Øª Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=1024,
            temperature=0.5
        )
        
        return await self.generate(request)


# ØªØ³Øª Ø³Ø±ÛŒØ¹
if __name__ == "__main__":
    async def test_llm():
        config = {
            'mode': 'mcp',
            'mcp': {
                'api_url': 'http://localhost:5005',
                'timeout': 300,
                'retry': 3
            },
            'fallback_online': True,
            'online': {
                'provider': 'openai',
                'api_key_env': 'OPENAI_API_KEY',
                'model': 'gpt-4'
            }
        }
        
        wrapper = LLMWrapper(config)
        
        # ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø³Ø§Ø¯Ù‡
        response = await wrapper.generate_code(
            task_description="Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÛŒØ¨ÙˆÙ†Ø§Ú†ÛŒ",
            file_path="fibonacci.py"
        )
        
        if response.success:
            print(f"âœ… Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ ({response.provider.value}):")
            print(response.content[:500])
            print(f"\nâ±ï¸  Ù…Ø¯Øª Ø²Ù…Ø§Ù†: {response.duration:.2f}s")
            print(f"ğŸ¯ Tokens: {response.tokens_used}")
        else:
            print(f"âŒ Ø®Ø·Ø§: {response.error}")
    
    asyncio.run(test_llm())