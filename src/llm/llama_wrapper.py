"""
LLM Wrapper - Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§ (MCP, Offline, Online)
"""

import asyncio
import aiohttp
import time
from typing import Optional, Dict, Any, List
from enum import Enum
from dataclasses import dataclass
import json


class LLMProvider(Enum):
    """Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ú¯Ø§Ù† LLM"""
    MCP = "mcp"
    OFFLINE = "offline"
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class LLMRequest:
    """Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ LLM"""
    prompt: str
    max_tokens: int = 2048
    temperature: float = 0.7
    system_prompt: Optional[str] = None
    context: Optional[List[Dict[str, str]]] = None


@dataclass
class LLMResponse:
    """Ù¾Ø§Ø³Ø® Ø§Ø² LLM"""
    content: str
    model: str
    provider: LLMProvider
    tokens_used: int
    duration: float
    success: bool
    error: Optional[str] = None


class MCPClient:
    """Ú©Ù„Ø§ÛŒÙ†Øª MCP Server"""
    
    def __init__(self, api_url: str, timeout: int = 300, retry: int = 3):
        self.api_url = api_url
        self.timeout = timeout
        self.retry = retry
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ MCP"""
        start_time = time.time()
        
        payload = {
            "prompt": request.prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "system_prompt": request.system_prompt
        }
        
        for attempt in range(self.retry):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.api_url}/generate",
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=self.timeout)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            duration = time.time() - start_time
                            
                            return LLMResponse(
                                content=data.get('content', ''),
                                model=data.get('model', 'mcp-model'),
                                provider=LLMProvider.MCP,
                                tokens_used=data.get('tokens', 0),
                                duration=duration,
                                success=True
                            )
                        else:
                            error_text = await response.text()
                            raise Exception(f"MCP error: {response.status} - {error_text}")
            
            except Exception as e:
                if attempt == self.retry - 1:
                    duration = time.time() - start_time
                    return LLMResponse(
                        content='',
                        model='mcp-failed',
                        provider=LLMProvider.MCP,
                        tokens_used=0,
                        duration=duration,
                        success=False,
                        error=str(e)
                    )
                await asyncio.sleep(2 ** attempt)  # Exponential backoff


class OfflineLLM:
    """LLM Ø¢ÙÙ„Ø§ÛŒÙ† (LLaMA/StarCoder)"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„"""
        try:
            # Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯
            # Ù…Ø«Ù„Ø§Ù‹ llama-cpp-python ÛŒØ§ transformers
            print(f"â³ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø²: {self.model_path}")
            
            # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¨Ø§ llama.cpp ÛŒØ§ ctransformers
            # from llama_cpp import Llama
            # self.model = Llama(model_path=self.model_path)
            
            print("âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {e}")
            self.model = None
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ù…Ø¯Ù„ Ø¢ÙÙ„Ø§ÛŒÙ†"""
        if not self.model:
            return LLMResponse(
                content='',
                model='offline-not-loaded',
                provider=LLMProvider.OFFLINE,
                tokens_used=0,
                duration=0,
                success=False,
                error="Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª"
            )
        
        start_time = time.time()
        
        try:
            # TODO: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ
            # output = self.model(
            #     request.prompt,
            #     max_tokens=request.max_tokens,
            #     temperature=request.temperature
            # )
            
            # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
            await asyncio.sleep(1)
            output = "# Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ø§ LLM Ø¢ÙÙ„Ø§ÛŒÙ†\npass"
            
            duration = time.time() - start_time
            
            return LLMResponse(
                content=output,
                model='llama-3.1-7b',
                provider=LLMProvider.OFFLINE,
                tokens_used=len(output.split()),
                duration=duration,
                success=True
            )
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model='llama-failed',
                provider=LLMProvider.OFFLINE,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )


class OnlineLLM:
    """LLM Ø¢Ù†Ù„Ø§ÛŒÙ† (OpenAI/Anthropic)"""
    
    def __init__(self, provider: str, api_key: str, model: str):
        self.provider = provider
        self.api_key = api_key
        self.model = model
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ API Ø¢Ù†Ù„Ø§ÛŒÙ†"""
        start_time = time.time()
        
        if self.provider == "openai":
            return await self._generate_openai(request, start_time)
        elif self.provider == "anthropic":
            return await self._generate_anthropic(request, start_time)
        else:
            return LLMResponse(
                content='',
                model='unknown',
                provider=LLMProvider.OPENAI,
                tokens_used=0,
                duration=0,
                success=False,
                error=f"Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±: {self.provider}"
            )
    
    async def _generate_openai(self, request: LLMRequest, start_time: float) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ OpenAI API"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                
                messages = []
                if request.system_prompt:
                    messages.append({"role": "system", "content": request.system_prompt})
                
                if request.context:
                    messages.extend(request.context)
                
                messages.append({"role": "user", "content": request.prompt})
                
                payload = {
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature
                }
                
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['choices'][0]['message']['content']
                        tokens = data['usage']['total_tokens']
                        duration = time.time() - start_time
                        
                        return LLMResponse(
                            content=content,
                            model=self.model,
                            provider=LLMProvider.OPENAI,
                            tokens_used=tokens,
                            duration=duration,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"OpenAI error: {response.status} - {error_text}")
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model=self.model,
                provider=LLMProvider.OPENAI,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )
    
    async def _generate_anthropic(self, request: LLMRequest, start_time: float) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ø¨Ø§ Anthropic API"""
        try:
            async with aiohttp.ClientSession() as session:
                headers = {
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01",
                    "Content-Type": "application/json"
                }
                
                messages = []
                if request.context:
                    messages.extend(request.context)
                
                messages.append({"role": "user", "content": request.prompt})
                
                payload = {
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature
                }
                
                if request.system_prompt:
                    payload["system"] = request.system_prompt
                
                async with session.post(
                    "https://api.anthropic.com/v1/messages",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=120)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = data['content'][0]['text']
                        tokens = data['usage']['input_tokens'] + data['usage']['output_tokens']
                        duration = time.time() - start_time
                        
                        return LLMResponse(
                            content=content,
                            model=self.model,
                            provider=LLMProvider.ANTHROPIC,
                            tokens_used=tokens,
                            duration=duration,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        raise Exception(f"Anthropic error: {response.status} - {error_text}")
        
        except Exception as e:
            duration = time.time() - start_time
            return LLMResponse(
                content='',
                model=self.model,
                provider=LLMProvider.ANTHROPIC,
                tokens_used=0,
                duration=duration,
                success=False,
                error=str(e)
            )


class LLMWrapper:
    """Ø±Ø§Ø¨Ø· ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… LLMÙ‡Ø§"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.mode = config.get('mode', 'mcp')
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§
        self.mcp_client = None
        self.offline_llm = None
        self.online_llm = None
        
        self._setup_clients()
    
    def _setup_clients(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù„Ø§ÛŒÙ†Øªâ€ŒÙ‡Ø§"""
        # MCP
        if self.mode == 'mcp' or self.config.get('fallback_online'):
            mcp_config = self.config.get('mcp', {})
            self.mcp_client = MCPClient(
                api_url=mcp_config.get('api_url', 'http://localhost:5005'),
                timeout=mcp_config.get('timeout', 300),
                retry=mcp_config.get('retry', 3)
            )
        
        # Offline
        if self.mode == 'offline':
            offline_config = self.config.get('offline_model', {})
            self.offline_llm = OfflineLLM(
                model_path=offline_config.get('path', './models/model.gguf')
            )
        
        # Online (Fallback)
        if self.config.get('fallback_online'):
            online_config = self.config.get('online', {})
            import os
            api_key = os.getenv(online_config.get('api_key_env', 'OPENAI_API_KEY'))
            
            self.online_llm = OnlineLLM(
                provider=online_config.get('provider', 'openai'),
                api_key=api_key or '',
                model=online_config.get('model', 'gpt-4')
            )
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM"""
        
        # ØªÙ„Ø§Ø´ Ø¨Ø§ MCP
        if self.mode == 'mcp' and self.mcp_client:
            response = await self.mcp_client.generate(request)
            if response.success:
                return response
            
            print(f"âš ï¸  MCP Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {response.error}")
        
        # ØªÙ„Ø§Ø´ Ø¨Ø§ Offline
        if self.mode == 'offline' and self.offline_llm:
            response = await self.offline_llm.generate(request)
            if response.success:
                return response
            
            print(f"âš ï¸  Offline LLM Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯: {response.error}")
        
        # Fallback Ø¨Ù‡ Online
        if self.config.get('fallback_online') and self.online_llm:
            print("ğŸ”„ Fallback Ø¨Ù‡ API Ø¢Ù†Ù„Ø§ÛŒÙ†...")
            response = await self.online_llm.generate(request)
            return response
        
        # Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯Ù†Ø¯
        return LLMResponse(
            content='',
            model='none',
            provider=LLMProvider.MCP,
            tokens_used=0,
            duration=0,
            success=False,
            error="Ù‡ÛŒÚ† LLM Ù…ÙˆÙÙ‚ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
        )
    
    async def generate_code(
        self,
        task_description: str,
        file_path: str,
        context: Optional[str] = None
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© task Ø®Ø§Øµ"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ Ù…Ø§Ù‡Ø± Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ú©Ø¯ Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.
Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. Ú©Ø¯ Ø¨Ø§ÛŒØ¯ Ú©Ø§Ù…Ù„ØŒ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ùˆ Ø¨Ø¯ÙˆÙ† Ø®Ø·Ø§ Ø¨Ø§Ø´Ø¯
2. Ø§Ø² type hints Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. docstring Ø¨Ø±Ø§ÛŒ ØªÙˆØ§Ø¨Ø¹ Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
4. error handling Ù…Ù†Ø§Ø³Ø¨ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯
5. Ú©Ø¯ Ø¨Ø§ÛŒØ¯ ØªÙ…ÛŒØ² Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯ (PEP 8)
6. ÙÙ‚Ø· Ú©Ø¯ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª Ø§Ø¶Ø§ÙÛŒ"""
        
        prompt = f"""Task: {task_description}
File: {file_path}

{"Context:\n" + context if context else ""}

Ù„Ø·ÙØ§Ù‹ Ú©Ø¯ Ú©Ø§Ù…Ù„ Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=4096,
            temperature=0.3
        )
        
        return await self.generate(request)
    
    async def generate_tests(
        self,
        code: str,
        file_path: str
    ) -> LLMResponse:
        """ØªÙˆÙ„ÛŒØ¯ ØªØ³Øª Ø¨Ø±Ø§ÛŒ Ú©Ø¯"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© ØªØ³Øªâ€ŒÙ†ÙˆÛŒØ³ Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯.
Ù‚ÙˆØ§Ù†ÛŒÙ†:
1. ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ú©Ø§Ù…Ù„ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
2. Ø§Ø² pytest Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
3. Ù…ÙˆØ§Ø±Ø¯ Ù…Ø±Ø²ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ø¯Ù‡ÛŒØ¯
4. ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ø§ÛŒØ¯ Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø§Ø´Ù†Ø¯
5. docstring Ø¨Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯"""
        
        prompt = f"""Ú©Ø¯ Ø²ÛŒØ± Ø±Ø§ ØªØ³Øª Ú©Ù†ÛŒØ¯:

```python
{code}
```

File path: {file_path}

Ù„Ø·ÙØ§Ù‹ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ pytest Ú©Ø§Ù…Ù„ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=2048,
            temperature=0.3
        )
        
        return await self.generate(request)
    
    async def review_code(self, code: str) -> LLMResponse:
        """Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ø¯"""
        
        system_prompt = """Ø´Ù…Ø§ ÛŒÚ© code reviewer Ù…ØªØ®ØµØµ Ù‡Ø³ØªÛŒØ¯.
Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ù‡ÛŒØ¯."""
        
        prompt = f"""Ú©Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:

```python
{code}
```

Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ Ø±Ø§ Ù„ÛŒØ³Øª Ú©Ù†ÛŒØ¯:"""
        
        request = LLMRequest(
            prompt=prompt,
            system_prompt=system_prompt,
            max_tokens=1024,
            temperature=0.5
        )
        
        return await self.generate(request)


# ØªØ³Øª Ø³Ø±ÛŒØ¹
if __name__ == "__main__":
    async def test_llm():
        config = {
            'mode': 'mcp',
            'mcp': {
                'api_url': 'http://localhost:5005',
                'timeout': 300,
                'retry': 3
            },
            'fallback_online': True,
            'online': {
                'provider': 'openai',
                'api_key_env': 'OPENAI_API_KEY',
                'model': 'gpt-4'
            }
        }
        
        wrapper = LLMWrapper(config)
        
        # ØªÙˆÙ„ÛŒØ¯ Ú©Ø¯ Ø³Ø§Ø¯Ù‡
        response = await wrapper.generate_code(
            task_description="Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ ÙÛŒØ¨ÙˆÙ†Ø§Ú†ÛŒ",
            file_path="fibonacci.py"
        )
        
        if response.success:
            print(f"âœ… Ú©Ø¯ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯ ({response.provider.value}):")
            print(response.content[:500])
            print(f"\nâ±ï¸  Ù…Ø¯Øª Ø²Ù…Ø§Ù†: {response.duration:.2f}s")
            print(f"ğŸ¯ Tokens: {response.tokens_used}")
        else:
            print(f"âŒ Ø®Ø·Ø§: {response.error}")
    
    asyncio.run(test_llm())